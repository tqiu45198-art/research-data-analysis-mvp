import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from scipy import stats
import warnings
import io
import re
from datetime import datetime
# æ ¸å¿ƒä¿®æ”¹ï¼šç”¨OpenAIå…¼å®¹å®¢æˆ·ç«¯è°ƒç”¨DeepSeekï¼ˆ2026å®˜æ–¹æ¨èï¼‰
from openai import OpenAI

# åŸºç¡€é…ç½®
warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
st.set_page_config(page_title="ç§‘ç ”æ•°æ®åˆ†æå¹³å°", page_icon="ğŸ“Š", layout="wide", initial_sidebar_state="expanded")

# æ ¸å¿ƒä¾èµ–å¯¼å…¥ï¼ˆä¿ç•™åŸæœ‰åˆ†æåº“ï¼‰
try:
    from scipy.stats import chi2_contingency, ttest_1samp, ttest_ind, ttest_rel, ks_2samp, mannwhitneyu, kruskal, friedmanchisquare, wilcoxon
    from statsmodels.stats.proportion import binom_test as sm_binom_test
    from statsmodels.formula.api import ols
    from statsmodels.stats.anova import anova_lm
    from statsmodels.stats.multicomp import pairwise_tukeyhsd
    from sklearn.cluster import KMeans
    from sklearn.preprocessing import StandardScaler, LabelEncoder
    from sklearn.linear_model import LinearRegression, LogisticRegression
    from sklearn.metrics import r2_score, classification_report
except ImportError as e:
    st.error(f"åˆ†æåº“å¯¼å…¥å¤±è´¥ï¼š{e}ï¼Œè¯·æ£€æŸ¥requirements.txt")

# ---------------------- æ ¸å¿ƒä¿®æ”¹ï¼š2026ç‰ˆDeepSeek APIè°ƒç”¨å‡½æ•°ï¼ˆé€‚é…Streamlit Cloudï¼‰ ----------------------
def call_deepseek_api(prompt, model="deepseek-chat", temperature=0.7):
    """
    2026å¹´DeepSeek APIè°ƒç”¨è§„èŒƒï¼ˆOpenAIå…¼å®¹å®¢æˆ·ç«¯+æµå¼è¾“å‡º+äº‘ç«¯å¯†é’¥ï¼‰
    :param prompt: æç¤ºè¯
    :param model: 2026ä¸»æµæ¨¡å‹ deepseek-chat/deepseek-reasoner
    :param temperature: ç”Ÿæˆéšæœºæ€§0-1
    :return: æµå¼ç”Ÿæˆå™¨/é”™è¯¯æç¤º
    """
    # 1. è¯»å–Streamlit Cloud Secretsä¸­çš„APIå¯†é’¥ï¼ˆæ ¸å¿ƒé€‚é…ï¼‰
    if "DEEPSEEK_API_KEY" not in st.secrets:
        return iter(["âŒ æœªé…ç½®APIå¯†é’¥ï¼šè¯·åœ¨Streamlit Cloud â†’ Settings â†’ Secretsä¸­æ·»åŠ  DEEPSEEK_API_KEY = 'ä½ çš„å¯†é’¥'"])
    
    api_key = st.secrets["DEEPSEEK_API_KEY"]
    # 2. åˆå§‹åŒ–OpenAIå…¼å®¹å®¢æˆ·ç«¯ï¼Œé…ç½®2026å®˜æ–¹Base URLï¼ˆæ ¸å¿ƒé€‚é…ï¼‰
    try:
        client = OpenAI(
            api_key=api_key,
            base_url="https://api.deepseek.com/v1"  # 2026å¹´DeepSeekå®˜æ–¹OpenAIå…¼å®¹åœ°å€
        )
    except Exception as e:
        return iter([f"âŒ å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}"])
    
    # 3. æ„é€ è¯·æ±‚ä½“ï¼ŒæŒ‰2026è§„èŒƒé…ç½®
    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=temperature,
            max_tokens=2048,
            stream=True  # å¼€å¯æµå¼è¾“å‡ºï¼Œè§£å†³æµ·å¤–ç½‘ç»œè¶…æ—¶ï¼ˆæ ¸å¿ƒé€‚é…ï¼‰
        )
        # æµå¼ç”Ÿæˆç»“æœï¼Œé€‚é…Streamlitè¾“å‡º
        def stream_generator():
            for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
        return stream_generator()
    # 4. æ•è·2026å¹´å¸¸è§é”™è¯¯ï¼ˆæ¨¡å‹ä¸å­˜åœ¨/å¯†é’¥æ— æ•ˆ/æœåŠ¡å™¨ç¹å¿™ï¼‰
    except client.BadRequestError as e:
        if "model_not_found" in str(e):
            return iter(["âŒ æ¨¡å‹ä¸å­˜åœ¨ï¼š2026å¹´ä¸»æµæ¨¡å‹ä¸º deepseek-chat / deepseek-reasoner"])
        return iter([f"âŒ è¯·æ±‚å‚æ•°é”™è¯¯ï¼š{str(e)}"])
    except client.UnauthorizedError:
        return iter(["âŒ APIå¯†é’¥æ— æ•ˆï¼šè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®/æœªè¿‡æœŸï¼ˆ2026å¹´å¯†é’¥æ ¼å¼ä¸ºsk-å¼€å¤´ï¼‰"])
    except client.ServiceUnavailableError:
        return iter(["âŒ DeepSeekæœåŠ¡å™¨ç¹å¿™ï¼š2026å¹´ç”¨æˆ·é‡æ¿€å¢ï¼Œå»ºè®®ç¨åé‡è¯•ï¼ˆå¯å…³æ³¨DeepSeekå®˜ç½‘çŠ¶æ€ï¼‰"])
    except TimeoutError:
        return iter(["âŒ ç½‘ç»œè¶…æ—¶ï¼šStreamlit Cloudæµ·å¤–æœåŠ¡å™¨è®¿é—®å»¶è¿Ÿï¼Œæµå¼è¾“å‡ºå·²ä¼˜åŒ–ï¼Œä»è¶…æ—¶è¯·ç¨åè¯•"])
    except Exception as e:
        return iter([f"âŒ APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}"])

# ---------------------- åŸæœ‰æ ¸å¿ƒåˆ†æå‡½æ•°ï¼ˆå®Œå…¨ä¿ç•™ï¼Œæ— ä¿®æ”¹ï¼‰ ----------------------
def load_and_clean_data(file):
    encodings = ['utf-8-sig', 'gbk', 'utf-8', 'gb2312']
    seps = [',', '\t', ';']
    try:
        file_content = file.read()
        file.seek(0)
        df = None
        if file.name.endswith(".csv"):
            for encoding in encodings:
                for sep in seps:
                    try:
                        df = pd.read_csv(file, encoding=encoding, sep=sep, on_bad_lines='skip')
                        break
                    except:
                        continue
                if df is not None:
                    break
            if df is None:
                from csv import Sniffer
                sample = file_content[:4096].decode('utf-8-sig', errors='replace')
                delimiter = Sniffer().sniff(sample).delimiter
                df = pd.read_csv(file, encoding='utf-8-sig', sep=delimiter, on_bad_lines='skip')
        else:
            df = pd.read_excel(file, engine='openpyxl')
        df.columns = [re.sub(r'[^\w\s\u4e00-\u9fa5/]', '', str(col)).strip() for col in df.columns]
        df.columns = [col if col else f"col_{i}" for i, col in enumerate(df.columns)]
        return df
    except Exception as e:
        st.error(f"æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{str(e)}")
        return None

def identify_variable_types(df):
    numeric_cols = []
    categorical_cols = []
    binary_categorical_cols = []
    datetime_cols = []
    for col in df.columns:
        if any(fmt in col.lower() for fmt in ['date', 'time', '2016', '2017', '2018', '2019', '2020']):
            try:
                df[col] = pd.to_datetime(df[col])
                datetime_cols.append(col)
                continue
            except:
                pass
        try:
            df[col] = pd.to_numeric(df[col], errors='raise')
            numeric_cols.append(col)
        except:
            categorical_cols.append(col)
            if df[col].nunique() == 2:
                binary_categorical_cols.append(col)
    return {'numeric': numeric_cols, 'categorical': categorical_cols, 'binary_categorical': binary_categorical_cols, 'datetime': datetime_cols}

def frequency_analysis(df, categorical_cols):
    freq_dict = {}
    for col in categorical_cols:
        freq = df[col].value_counts()
        freq_pct = df[col].value_counts(normalize=True) * 100
        freq_df = pd.DataFrame({'é¢‘æ•°': freq, 'é¢‘ç‡(%)': freq_pct.round(2)})
        freq_dict[col] = freq_df
    return freq_dict

def descriptive_analysis(df, numeric_cols):
    desc_df = df[numeric_cols].describe().T
    desc_df['ç¼ºå¤±å€¼'] = df[numeric_cols].isnull().sum()
    desc_df['ç¼ºå¤±ç‡(%)'] = (desc_df['ç¼ºå¤±å€¼'] / len(df) * 100).round(2)
    desc_df['ååº¦'] = df[numeric_cols].skew().round(3)
    desc_df['å³°åº¦'] = df[numeric_cols].kurt().round(3)
    return desc_df

def contingency_table_analysis(df, col1, col2):
    cont_table = pd.crosstab(df[col1], df[col2])
    chi2, p, dof, expected = chi2_contingency(cont_table)
    cramers_v = np.sqrt(chi2 / (len(df) * min(cont_table.shape[0]-1, cont_table.shape[1]-1)))
    return {'è”åˆ—è¡¨': cont_table, 'å¡æ–¹å€¼': chi2.round(3), 'på€¼': p.round(4), 'è‡ªç”±åº¦': dof, 'å…‹è±å§†Vç³»æ•°': cramers_v.round(3)}

def t_test_onesample(df, numeric_col, popmean):
    data = df[numeric_col].dropna()
    t_stat, p_value = ttest_1samp(data, popmean)
    return {'tå€¼': t_stat.round(3), 'på€¼': p_value.round(4), 'å‡å€¼': data.mean().round(2), 'æ ·æœ¬é‡': len(data)}

def t_test_independent(df, numeric_col, group_col):
    groups = df[group_col].unique()
    if len(groups) != 2:
        return {'error': 'åˆ†ç»„å˜é‡å¿…é¡»ä¸ºäºŒåˆ†ç±»'}
    group1 = df[df[group_col] == groups[0]][numeric_col].dropna()
    group2 = df[df[group_col] == groups[1]][numeric_col].dropna()
    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)
    return {'tå€¼': t_stat.round(3), 'på€¼': p_value.round(4), f'{groups[0]}å‡å€¼': group1.mean().round(2), f'{groups[1]}å‡å€¼': group2.mean().round(2), f'{groups[0]}æ ·æœ¬é‡': len(group1), f'{groups[1]}æ ·æœ¬é‡': len(group2)}

def nonparametric_test(df, test_type, numeric_col, group_col=None):
    if test_type == 'å•æ ·æœ¬K-Sæ£€éªŒ':
        data = df[numeric_col].dropna()
        ks_stat, p_value = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
        return {'KSç»Ÿè®¡é‡': ks_stat.round(3), 'på€¼': p_value.round(4)}
    elif test_type == 'ä¸¤ç‹¬ç«‹æ ·æœ¬Mann-Whitney Uæ£€éªŒ':
        groups = df[group_col].unique()
        if len(groups) != 2:
            return {'error': 'åˆ†ç»„å˜é‡å¿…é¡»ä¸ºäºŒåˆ†ç±»'}
        group1 = df[df[group_col] == groups[0]][numeric_col].dropna()
        group2 = df[df[group_col] == groups[1]][numeric_col].dropna()
        u_stat, p_value = mannwhitneyu(group1, group2)
        return {'Uå€¼': u_stat.round(3), 'på€¼': p_value.round(4)}
    elif test_type == 'äºŒé¡¹åˆ†å¸ƒæ£€éªŒ':
        data = df[numeric_col].dropna()
        success = sum(data == 1)
        n = len(data)
        p_value = sm_binom_test(success, n, prop=0.5)
        return {'æˆåŠŸæ¬¡æ•°': success, 'æ€»æ¬¡æ•°': n, 'på€¼': p_value.round(4)}
    return {'error': 'æ— æ•ˆæ£€éªŒç±»å‹'}

def anova_analysis(df, formula, anova_type):
    model = ols(formula, data=df).fit()
    anova_result = anova_lm(model, typ=2)
    tukey = pairwise_tukeyhsd(df[formula.split('~')[0]], df[formula.split('~')[1].split('+')[0]], alpha=0.05)
    return {'æ–¹å·®åˆ†æè¡¨': anova_result, 'äº‹åæ£€éªŒ(Tukey)': tukey.summary()}

def correlation_analysis(df, cols, corr_type='pearson'):
    corr_df = df[cols].dropna()
    if corr_type == 'pearson':
        corr_matrix = corr_df.corr(method='pearson')
        p_matrix = pd.DataFrame(np.ones_like(corr_matrix), index=corr_matrix.index, columns=corr_matrix.columns)
        for col1 in cols:
            for col2 in cols:
                if col1 != col2:
                    corr, p = stats.pearsonr(corr_df[col1], corr_df[col2])
                    p_matrix.loc[col1, col2] = round(p, 4)
    elif corr_type == 'spearman':
        corr_matrix = corr_df.corr(method='spearman')
        p_matrix = pd.DataFrame(np.ones_like(corr_matrix), index=corr_matrix.index, columns=corr_matrix.columns)
        for col1 in cols:
            for col2 in cols:
                if col1 != col2:
                    corr, p = stats.spearmanr(corr_df[col1], corr_df[col2])
                    p_matrix.loc[col1, col2] = round(p, 4)
    return {'ç›¸å…³çŸ©é˜µ': corr_matrix.round(3), 'på€¼çŸ©é˜µ': p_matrix}

def regression_analysis(df, target, features, reg_type):
    X = df[features].dropna()
    y = df[target][X.index].dropna()
    X = X.loc[y.index]
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    if reg_type == 'çº¿æ€§å›å½’':
        model = LinearRegression().fit(X_scaled, y)
        y_pred = model.predict(X_scaled)
        r2 = r2_score(y, y_pred)
        coef = pd.DataFrame({'ç‰¹å¾': features, 'ç³»æ•°': model.coef_.round(3), 'æˆªè·': [model.intercept_.round(3)]*len(features)})
        return {'RÂ²': r2.round(3), 'ç³»æ•°è¡¨': coef}
    elif reg_type == 'äºŒåˆ†ç±»Logisticå›å½’':
        le = LabelEncoder()
        y_encoded = le.fit_transform(y)
        model = LogisticRegression(max_iter=1000).fit(X_scaled, y_encoded)
        y_pred = model.predict(X_scaled)
        report = classification_report(y_encoded, y_pred, output_dict=True)
        coef = pd.DataFrame({'ç‰¹å¾': features, 'ç³»æ•°': model.coef_[0].round(3), 'æˆªè·': [model.intercept_[0].round(3)]*len(features)})
        return {'åˆ†ç±»æŠ¥å‘Š': report, 'ç³»æ•°è¡¨': coef}
    return {'error': 'æ— æ•ˆå›å½’ç±»å‹'}

def cluster_analysis(df, cols, n_clusters=3):
    X = df[cols].dropna()
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X_scaled)
    df_cluster = X.copy()
    df_cluster['èšç±»ç»“æœ'] = kmeans.labels_
    centroids = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=cols).round(2)
    return {'èšç±»ç»“æœ': df_cluster, 'èšç±»ä¸­å¿ƒ': centroids}

def plot_chart(df, plot_type, x_col, y_col=None, group_col=None):
    if plot_type == 'æ¡å½¢å›¾':
        fig = px.bar(df, x=x_col, y=y_col, color=group_col, barmode='group', title=f'{x_col} - {y_col}')
    elif plot_type == 'æŠ˜çº¿å›¾':
        fig = px.line(df, x=x_col, y=y_col, color=group_col, title=f'{x_col} - {y_col}')
    elif plot_type == 'é¥¼å›¾':
        fig = px.pie(df, names=x_col, values=y_col, title=f'{x_col} åˆ†å¸ƒ')
    elif plot_type == 'ç®±å›¾':
        fig = px.box(df, x=x_col, y=y_col, color=group_col, title=f'{x_col} - {y_col} åˆ†å¸ƒ')
    fig.update_layout(width=800, height=500)
    return fig

# ---------------------- é¡µé¢ä¸»ä½“ï¼ˆåˆ é™¤ä¾§è¾¹æ APIè¾“å…¥æ¡†ï¼Œé€‚é…äº‘ç«¯Secretsï¼‰ ----------------------
st.title("ç§‘ç ”æ•°æ®åˆ†æå¹³å°")
st.divider()

# ä¾§è¾¹æ ï¼ˆä»…ä¿ç•™æ•°æ®ä¸Šä¼ /åˆå¹¶ï¼Œåˆ é™¤åŸAPIè¾“å…¥æ¡†ï¼‰
with st.sidebar:
    st.markdown("## ğŸ“¥ æ•°æ®ä¸Šä¼ ")
    uploaded_files = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ï¼ˆCSV/Excelï¼Œæ”¯æŒå¤šæ–‡ä»¶ï¼‰", type=["xlsx", "csv"], accept_multiple_files=True)
    df = None
    var_types = None
    if uploaded_files:
        selected_file_names = st.multiselect("é€‰æ‹©åˆ†ææ–‡ä»¶", [f.name for f in uploaded_files], default=[uploaded_files[0].name])
        selected_files = [f for f in uploaded_files if f.name in selected_file_names]
        
        df_dict = {}
        for file in selected_files:
            df_temp = load_and_clean_data(file)
            if df_temp is not None:
                df_dict[file.name] = df_temp
                st.success(f"{file.name} ä¸Šä¼ æˆåŠŸ ({len(df_temp)}è¡ŒÃ—{len(df_temp.columns)}åˆ—)")
        
        # å¤šæ–‡ä»¶åˆå¹¶
        if len(df_dict) >= 2:
            base_file = st.selectbox("åŸºç¡€æ–‡ä»¶", list(df_dict.keys()))
            df = df_dict[base_file]
            for other_file in [f for f in df_dict.keys() if f != base_file]:
                df_other = df_dict[other_file]
                common_cols = [col for col in df.columns if col in df_other.columns]
                base_key = st.selectbox(f"åŸºç¡€å…³è”å­—æ®µ", common_cols if common_cols else df.columns, key=f"base_{other_file}")
                join_key = st.selectbox(f"å…³è”å­—æ®µ", common_cols if common_cols else df_other.columns, key=f"join_{other_file}")
                join_type = st.selectbox(f"åˆå¹¶æ–¹å¼", ['å·¦è¿æ¥', 'å³è¿æ¥', 'å†…è¿æ¥', 'å¤–è¿æ¥'], key=f"type_{other_file}")
                join_map = {'å·¦è¿æ¥':'left', 'å³è¿æ¥':'right', 'å†…è¿æ¥':'inner', 'å¤–è¿æ¥':'outer'}
                if st.button(f"åˆå¹¶{other_file}", key=f"btn_{other_file}"):
                    df = pd.merge(df, df_other, left_on=base_key, right_on=join_key, how=join_map[join_type], suffixes=("", f"_{other_file.split('.')[0]}"))
                    st.success(f"åˆå¹¶åï¼š{len(df)}è¡ŒÃ—{len(df.columns)}åˆ—")
        else:
            df = df_dict[list(df_dict.keys())[0]] if df_dict else None
        
        # æ•°æ®æ¦‚å†µ
        if df is not None:
            var_types = identify_variable_types(df)
            st.markdown("## ğŸ“Š æ•°æ®æ¦‚å†µ")
            st.write(f"è§„æ¨¡ï¼š{len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            st.write(f"æ•°å€¼å‹å˜é‡ï¼š{len(var_types['numeric'])}ä¸ª")
            st.write(f"åˆ†ç±»å‹å˜é‡ï¼š{len(var_types['categorical'])}ä¸ª")

# ä¸»å†…å®¹åŒºï¼ˆä¿ç•™åŸæœ‰7ä¸ªåˆ†ææ ‡ç­¾é¡µ+AIåˆ†ææ ‡ç­¾é¡µï¼Œæµå¼è¾“å‡ºAIç»“æœï¼‰
if df is not None and var_types is not None:
    # æå–æ•°æ®æ¦‚å†µï¼ˆä¼ ç»™AIï¼Œä¿æŠ¤éšç§ï¼‰
    data_overview = f"""
    æœ¬æ¬¡åˆ†ææ•°æ®æ¦‚å†µï¼š
    1. æ•°æ®è§„æ¨¡ï¼š{len(df)}è¡Œ Ã— {len(df.columns)}åˆ—
    2. æ•°å€¼å‹å˜é‡ï¼š{', '.join(var_types['numeric']) if var_types['numeric'] else 'æ— '}
    3. åˆ†ç±»å‹å˜é‡ï¼š{', '.join(var_types['categorical']) if var_types['categorical'] else 'æ— '}
    4. äºŒåˆ†ç±»å˜é‡ï¼š{', '.join(var_types['binary_categorical']) if var_types['binary_categorical'] else 'æ— '}
    5. ç¼ºå¤±å€¼æ€»æ•°ï¼š{df.isnull().sum().sum()}ä¸ªï¼Œæ•´ä½“ç¼ºå¤±ç‡ï¼š{(df.isnull().sum().sum()/(df.shape[0]*df.shape[1]))*100:.2f}%
    """
    # åˆ†ææ ‡ç­¾é¡µï¼ˆåŸæœ‰7ä¸ª+AIåˆ†æï¼‰
    tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([
        "æ•°æ®å¤„ç†", "åŸºæœ¬ç»Ÿè®¡", "å‡å€¼æ£€éªŒ", "æ–¹å·®åˆ†æ", "ç›¸å…³åˆ†æ", "å›å½’åˆ†æ", "å¯è§†åŒ–", "AIåˆ†æ"
    ])

    # æ ‡ç­¾é¡µ1-7ï¼šåŸæœ‰åˆ†æåŠŸèƒ½ï¼ˆå®Œå…¨ä¿ç•™ï¼Œå·²åšå‚æ•°æ ¡éªŒï¼‰
    with tab1:
        st.subheader("æ•°æ®å¤„ç†")
        sort_col = st.selectbox("æ’åºå­—æ®µ", df.columns, key='sort')
        sort_asc = st.radio("æ’åºæ–¹å¼", ['å‡åº', 'é™åº'], key='sort_asc')
        if st.button("æ‰§è¡Œæ’åº"):
            df_sorted = df.sort_values(by=sort_col, ascending=(sort_asc=='å‡åº'))
            st.dataframe(df_sorted.head(10), use_container_width=True)
        
        filter_col = st.selectbox("ç­›é€‰å­—æ®µ", df.columns, key='filter')
        filter_op = st.selectbox("è¿ç®—ç¬¦", ['>', '<', '>=', '<=', '==', '!='], key='filter_op')
        filter_val = st.text_input("ç­›é€‰å€¼", key='filter_val')
        if st.button("æ‰§è¡Œç­›é€‰"):
            try:
                if df[filter_col].dtype in [np.int64, np.float64]:
                    filter_val = float(filter_val)
                df_filtered = df.query(f"`{filter_col}` {filter_op} {filter_val}")
                st.success(f"ç­›é€‰åï¼š{len(df_filtered)}è¡Œ")
                st.dataframe(df_filtered.head(10), use_container_width=True)
            except:
                st.error("ç­›é€‰æ¡ä»¶é”™è¯¯ï¼Œè¯·æ£€æŸ¥å€¼çš„ç±»å‹")
        
        group_col = st.selectbox("åˆ†ç»„å­—æ®µ", var_types['categorical'], key='group', disabled=not var_types['categorical'])
        agg_col = st.selectbox("æ±‡æ€»å­—æ®µ", var_types['numeric'], key='agg', disabled=not var_types['numeric'])
        agg_func = st.selectbox("æ±‡æ€»æ–¹å¼", ['å‡å€¼', 'æ±‚å’Œ', 'è®¡æ•°', 'æœ€å¤§å€¼', 'æœ€å°å€¼'], key='agg_func')
        agg_map = {'å‡å€¼':'mean', 'æ±‚å’Œ':'sum', 'è®¡æ•°':'count', 'æœ€å¤§å€¼':'max', 'æœ€å°å€¼':'min'}
        if st.button("æ‰§è¡Œåˆ†ç±»æ±‡æ€»", disabled=not (group_col and agg_col)):
            df_agg = df.groupby(group_col)[agg_col].agg(agg_map[agg_func]).round(2)
            st.dataframe(df_agg, use_container_width=True)

    with tab2:
        st.subheader("åŸºæœ¬ç»Ÿè®¡")
        freq_cols = st.multiselect("é€‰æ‹©åˆ†ç±»å‹å˜é‡", var_types['categorical'], key='freq')
        if freq_cols and st.button("æ‰§è¡Œé¢‘æ•°åˆ†æ"):
            freq_dict = frequency_analysis(df, freq_cols)
            for col in freq_cols:
                st.subheader(col)
                st.dataframe(freq_dict[col], use_container_width=True)
        
        desc_cols = st.multiselect("é€‰æ‹©æ•°å€¼å‹å˜é‡", var_types['numeric'], key='desc')
        if desc_cols and st.button("æ‰§è¡Œæè¿°ç»Ÿè®¡"):
            desc_df = descriptive_analysis(df, desc_cols)
            st.dataframe(desc_df, use_container_width=True)
        
        if len(var_types['categorical'])>=2:
            cont_col1 = st.selectbox("è¡Œå˜é‡", var_types['categorical'], key='cont1')
            cont_col2 = st.selectbox("åˆ—å˜é‡", var_types['categorical'], key='cont2')
            if st.button("æ‰§è¡Œè”åˆ—è¡¨+å¡æ–¹æ£€éªŒ"):
                cont_res = contingency_table_analysis(df, cont_col1, cont_col2)
                st.subheader("è”åˆ—è¡¨")
                st.dataframe(cont_res['è”åˆ—è¡¨'], use_container_width=True)
                st.write(f"å¡æ–¹å€¼ï¼š{cont_res['å¡æ–¹å€¼']} | på€¼ï¼š{cont_res['på€¼']} | è‡ªç”±åº¦ï¼š{cont_res['è‡ªç”±åº¦']} | å…‹è±å§†Vç³»æ•°ï¼š{cont_res['å…‹è±å§†Vç³»æ•°']}")

    with tab3:
        st.subheader("å‡å€¼æ£€éªŒ")
        onesamp_col = st.selectbox("æ£€éªŒå˜é‡", var_types['numeric'], key='onesamp', disabled=not var_types['numeric'])
        popmean = st.number_input("æ€»ä½“å‡å€¼", value=0.0, key='popmean')
        if st.button("æ‰§è¡Œå•æ ·æœ¬tæ£€éªŒ", disabled=not onesamp_col):
            onesamp_res = t_test_onesample(df, onesamp_col, popmean)
            st.write(f"tå€¼ï¼š{onesamp_res['tå€¼']} | på€¼ï¼š{onesamp_res['på€¼']} | æ ·æœ¬å‡å€¼ï¼š{onesamp_res['å‡å€¼']} | æ ·æœ¬é‡ï¼š{onesamp_res['æ ·æœ¬é‡']}")
        
        ind_col = st.selectbox("æ£€éªŒå˜é‡", var_types['numeric'], key='ind', disabled=not var_types['numeric'])
        ind_group = st.selectbox("åˆ†ç»„å˜é‡", var_types['categorical'], key='ind_group', disabled=not var_types['categorical'])
        if st.button("æ‰§è¡Œä¸¤ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ", disabled=not (ind_col and ind_group)):
            ind_res = t_test_independent(df, ind_col, ind_group)
            if 'error' in ind_res:
                st.error(ind_res['error'])
            else:
                st.write(f"tå€¼ï¼š{ind_res['tå€¼']} | på€¼ï¼š{ind_res['på€¼']}")
                st.write(f"{list(ind_res.keys())[2]}ï¼š{ind_res[list(ind_res.keys())[2]]} | {list(ind_res.keys())[3]}ï¼š{ind_res[list(ind_res.keys())[3]]}")
        
        test_type = st.selectbox("éå‚æ•°æ£€éªŒç±»å‹", ['å•æ ·æœ¬K-Sæ£€éªŒ', 'äºŒé¡¹åˆ†å¸ƒæ£€éªŒ', 'ä¸¤ç‹¬ç«‹æ ·æœ¬Mann-Whitney Uæ£€éªŒ'], key='test_type')
        np_col = st.selectbox("æ£€éªŒå˜é‡", var_types['numeric'], key='np', disabled=not var_types['numeric'])
        np_group = st.selectbox("åˆ†ç»„å˜é‡", var_types['categorical'], key='np_group', disabled=test_type not in ['ä¸¤ç‹¬ç«‹æ ·æœ¬Mann-Whitney Uæ£€éªŒ'])
        if st.button("æ‰§è¡Œéå‚æ•°æ£€éªŒ", disabled=not np_col):
            np_res = nonparametric_test(df, test_type, np_col, np_group)
            if 'error' in np_res:
                st.error(np_res['error'])
            else:
                for k, v in np_res.items():
                    st.write(f"{k}ï¼š{v}")

    with tab4:
        st.subheader("æ–¹å·®åˆ†æ")
        if var_types['numeric'] and var_types['categorical']:
            anova_target = st.selectbox("å› å˜é‡ï¼ˆæ•°å€¼å‹ï¼‰", var_types['numeric'], key='anova_target')
            anova_factor = st.selectbox("å› ç´ å˜é‡ï¼ˆåˆ†ç±»å‹ï¼‰", var_types['categorical'], key='anova_factor')
            formula = f"{anova_target} ~ C({anova_factor})"
            if st.button("æ‰§è¡Œå•å› ç´ æ–¹å·®åˆ†æ+Tukeyäº‹åæ£€éªŒ"):
                anova_res = anova_analysis(df, formula, 'å•å› ç´ æ–¹å·®åˆ†æ')
                st.subheader("æ–¹å·®åˆ†æè¡¨")
                st.dataframe(anova_res['æ–¹å·®åˆ†æè¡¨'], use_container_width=True)
                st.subheader("Tukeyäº‹åæ£€éªŒç»“æœ")
                st.text(anova_res['äº‹åæ£€éªŒ(Tukey)'])

    with tab5:
        st.subheader("ç›¸å…³åˆ†æ")
        corr_type = st.selectbox("ç›¸å…³ç³»æ•°ç±»å‹", ['pearsonï¼ˆçš®å°”é€Šï¼Œé€‚ç”¨äºæ­£æ€ï¼‰', 'spearmanï¼ˆæ–¯çš®å°”æ›¼ï¼Œéå‚æ•°ï¼‰'], key='corr_type')
        corr_type_map = {'pearsonï¼ˆçš®å°”é€Šï¼Œé€‚ç”¨äºæ­£æ€ï¼‰':'pearson', 'spearmanï¼ˆæ–¯çš®å°”æ›¼ï¼Œéå‚æ•°ï¼‰':'spearman'}
        corr_cols = st.multiselect("é€‰æ‹©æ•°å€¼å‹å˜é‡ï¼ˆè‡³å°‘2ä¸ªï¼‰", var_types['numeric'], key='corr_cols')
        if len(corr_cols) < 2:
            st.warning("âš ï¸ è¯·é€‰æ‹©è‡³å°‘2ä¸ªæ•°å€¼å‹å˜é‡")
            st.button("æ‰§è¡Œç›¸å…³åˆ†æï¼ˆå«çƒ­åŠ›å›¾ï¼‰", disabled=True)
        else:
            if st.button("æ‰§è¡Œç›¸å…³åˆ†æï¼ˆå«çƒ­åŠ›å›¾ï¼‰"):
                corr_res = correlation_analysis(df, corr_cols, corr_type_map[corr_type])
                st.subheader("ç›¸å…³ç³»æ•°çŸ©é˜µ")
                st.dataframe(corr_res['ç›¸å…³çŸ©é˜µ'], use_container_width=True)
                st.subheader("på€¼çŸ©é˜µ")
                st.dataframe(corr_res['på€¼çŸ©é˜µ'], use_container_width=True)
                
                fig, ax = plt.subplots(figsize=(10, 8))
                im = ax.imshow(corr_res['ç›¸å…³çŸ©é˜µ'], cmap='RdBu_r', vmin=-1, vmax=1)
                ax.set_xticks(np.arange(len(corr_cols)))
                ax.set_yticks(np.arange(len(corr_cols)))
                ax.set_xticklabels(corr_cols, rotation=45, ha='right')
                ax.set_yticklabels(corr_cols)
                for i in range(len(corr_cols)):
                    for j in range(len(corr_cols)):
                        text = ax.text(j, i, corr_res['ç›¸å…³çŸ©é˜µ'].iloc[i, j], ha="center", va="center", color="black")
                cbar = ax.figure.colorbar(im, ax=ax)
                plt.tight_layout()
                st.pyplot(fig)

    with tab6:
        st.subheader("å›å½’åˆ†æ")
        reg_type = st.selectbox("å›å½’ç±»å‹", ['çº¿æ€§å›å½’', 'äºŒåˆ†ç±»Logisticå›å½’'], key='reg_type')
        reg_target = st.selectbox("å› å˜é‡", var_types['numeric'] if reg_type=='çº¿æ€§å›å½’' else var_types['binary_categorical'], key='reg_target')
        reg_features = st.multiselect("è‡ªå˜é‡ï¼ˆæ•°å€¼å‹ï¼‰", [col for col in var_types['numeric'] if col != reg_target], key='reg_features')
        if st.button("æ‰§è¡Œå›å½’åˆ†æ", disabled=not (reg_target and reg_features)):
            reg_res = regression_analysis(df, reg_target, reg_features, reg_type)
            if 'error' in reg_res:
                st.error(reg_res['error'])
            else:
                if reg_type == 'çº¿æ€§å›å½’':
                    st.write(f"ğŸ“Š æ¨¡å‹æ‹Ÿåˆåº¦ RÂ²ï¼š{reg_res['RÂ²']}")
                else:
                    st.write(f"ğŸ“Š æ¨¡å‹å‡†ç¡®ç‡ï¼š{reg_res['åˆ†ç±»æŠ¥å‘Š']['accuracy']:.3f}")
                st.subheader("ç³»æ•°è¡¨")
                st.dataframe(reg_res['ç³»æ•°è¡¨'], use_container_width=True)

    with tab7:
        st.subheader("å¯è§†åŒ–åˆ†æ")
        plot_type = st.selectbox("å›¾è¡¨ç±»å‹", ['æ¡å½¢å›¾', 'æŠ˜çº¿å›¾', 'é¥¼å›¾', 'ç®±å›¾'], key='plot_type')
        if plot_type in ['æ¡å½¢å›¾', 'æŠ˜çº¿å›¾', 'ç®±å›¾']:
            x_col = st.selectbox("Xè½´å˜é‡", df.columns, key='plot_x')
            y_col = st.selectbox("Yè½´å˜é‡ï¼ˆæ•°å€¼å‹ï¼‰", var_types['numeric'], key='plot_y')
            group_col = st.selectbox("åˆ†ç»„å˜é‡ï¼ˆå¯é€‰ï¼‰", [None] + var_types['categorical'], key='plot_group')
        else:
            x_col = st.selectbox("ç±»åˆ«å˜é‡", var_types['categorical'], key='plot_x_pie')
            y_col = st.selectbox("æ•°å€¼å˜é‡ï¼ˆç”¨äºå æ¯”ï¼‰", var_types['numeric'], key='plot_y_pie')
            group_col = None
        if st.button("ç”Ÿæˆå›¾è¡¨"):
            fig = plot_chart(df, plot_type, x_col, y_col, group_col)
            st.plotly_chart(fig, use_container_width=True)

    # AIåˆ†ææ ‡ç­¾é¡µï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šæµå¼è¾“å‡ºAIç»“æœï¼Œé€‚é…2026ç‰ˆAPIï¼‰
    with tab8:
        st.subheader("ğŸ¤– AI æ™ºèƒ½åˆ†æï¼ˆ2026 DeepSeekå®˜æ–¹ç‰ˆï¼‰")
        # å¯†é’¥é…ç½®æç¤º
        if "DEEPSEEK_API_KEY" not in st.secrets:
            st.warning("âš ï¸ è¯·å…ˆåœ¨ã€Streamlit Cloud â†’ Settings â†’ Secretsã€‘ä¸­é…ç½®ï¼šDEEPSEEK_API_KEY = 'ä½ çš„sk-å¼€å¤´å¯†é’¥'")
        else:
            st.success("âœ… APIå¯†é’¥å·²é…ç½®ï¼Œæ”¯æŒæµå¼è¾“å‡ºï¼ˆè§£å†³æµ·å¤–ç½‘ç»œè¶…æ—¶ï¼‰")
            st.markdown("---")
            # AIåŠŸèƒ½1ï¼šè‡ªåŠ¨æ•°æ®åˆ†æï¼ˆåŸºäºçœŸå®ç»Ÿè®¡ç»“æœï¼‰
with st.expander("ğŸ“‘ AIè‡ªåŠ¨æ•°æ®åˆ†æï¼ˆåŸºäºçœŸå®è®¡ç®—ç»“æœï¼‰", expanded=True):
    st.markdown("ä»£ç ä¼šå…ˆè‡ªåŠ¨æ‰§è¡ŒçœŸå®ç»Ÿè®¡åˆ†æï¼ŒAIä»…åŸºäºè¿™äº›çœŸå®ç»“æœç”ŸæˆæŠ¥å‘Šï¼ˆæ— å‡æ•°å€¼ï¼‰")
    if st.button("ğŸš€ å¼€å§‹AIè‡ªåŠ¨åˆ†æï¼ˆçœŸå®æ•°æ®ï¼‰"):
        with st.spinner("æ­£åœ¨æ‰§è¡ŒçœŸå®ç»Ÿè®¡åˆ†æï¼Œè¯·ç¨å€™..."):
            # ---------------------- æ­¥éª¤1ï¼šè‡ªåŠ¨æ‰§è¡ŒçœŸå®ç»Ÿè®¡åˆ†æï¼ˆè°ƒç”¨ç°æœ‰å‡½æ•°ï¼Œç»“æœ100%çœŸå®ï¼‰ ----------------------
            # 1. æè¿°ç»Ÿè®¡ï¼ˆçœŸå®ç»“æœï¼‰
            desc_res = descriptive_analysis(df, var_types['numeric']) if var_types['numeric'] else "æ— æ•°å€¼å‹å˜é‡"
            desc_text = "### æè¿°ç»Ÿè®¡ç»“æœ\n" + desc_res.to_string() if var_types['numeric'] else "æ— æ•°å€¼å‹å˜é‡"
            
            # 2. æ•°å€¼å˜é‡ç›¸å…³çŸ©é˜µï¼ˆçœŸå®ç»“æœï¼‰
            corr_res = correlation_analysis(df, var_types['numeric'], 'pearson') if len(var_types['numeric'])>=2 else "æ•°å€¼å‹å˜é‡ä¸è¶³2ä¸ª"
            corr_text = "### æ•°å€¼å˜é‡ç›¸å…³çŸ©é˜µï¼ˆPearsonï¼‰\n" + corr_res['ç›¸å…³çŸ©é˜µ'].to_string() if len(var_types['numeric'])>=2 else "æ•°å€¼å‹å˜é‡ä¸è¶³2ä¸ª"
            
            # 3. åˆ†ç±»å‹å˜é‡é¢‘æ•°ï¼ˆçœŸå®ç»“æœï¼‰
            freq_res = frequency_analysis(df, var_types['categorical']) if var_types['categorical'] else "æ— åˆ†ç±»å‹å˜é‡"
            freq_text = "### åˆ†ç±»å‹å˜é‡é¢‘æ•°ç»“æœ\n"
            if var_types['categorical']:
                for col in var_types['categorical']:
                    freq_text += f"\n{col}ï¼š\n" + freq_res[col].to_string()
            else:
                freq_text = "æ— åˆ†ç±»å‹å˜é‡"
            
            # 4. å…³é”®å‡å€¼æ£€éªŒï¼ˆè‹¥æœ‰äºŒåˆ†ç±»å˜é‡ï¼Œè‡ªåŠ¨åšä¸¤ç‹¬ç«‹æ ·æœ¬tæ£€éªŒï¼‰
            ttest_text = "### å‡å€¼æ£€éªŒç»“æœ\n"
            if var_types['binary_categorical'] and var_types['numeric']:
                group_col = var_types['binary_categorical'][0]  # å–ç¬¬ä¸€ä¸ªäºŒåˆ†ç±»å˜é‡
                test_col = var_types['numeric'][0]  # å–ç¬¬ä¸€ä¸ªæ•°å€¼å˜é‡
                ttest_res = t_test_independent(df, test_col, group_col)
                if 'error' not in ttest_res:
                    ttest_text += f"ä¸¤ç‹¬ç«‹æ ·æœ¬tæ£€éªŒï¼ˆ{test_col}æŒ‰{group_col}åˆ†ç»„ï¼‰ï¼š\n"
                    ttest_text += f"tå€¼={ttest_res['tå€¼']}ï¼Œpå€¼={ttest_res['på€¼']}ï¼Œ{list(ttest_res.keys())[2]}={ttest_res[list(ttest_res.keys())[2]]}ï¼Œ{list(ttest_res.keys())[3]}={ttest_res[list(ttest_res.keys())[3]]}"
            else:
                ttest_text += "æ— ç¬¦åˆæ¡ä»¶çš„äºŒåˆ†ç±»å˜é‡ï¼Œæœªæ‰§è¡Œå‡å€¼æ£€éªŒ"

            # ---------------------- æ­¥éª¤2ï¼šå°†çœŸå®ç»“æœæ•´ç†ä¸ºæç¤ºè¯ä¸Šä¸‹æ–‡ ----------------------
            real_stats_text = f"""ä»¥ä¸‹æ˜¯è¯¥æ•°æ®çš„çœŸå®ç»Ÿè®¡åˆ†æç»“æœï¼Œä½ åªèƒ½åŸºäºè¿™äº›ç»“æœç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œ**ç¦æ­¢ç¼–é€ ä»»ä½•æ•°å€¼**ï¼š
{desc_text}

{corr_text}

{freq_text}

{ttest_text}
"""

            # ---------------------- æ­¥éª¤3ï¼šè°ƒç”¨AIï¼ŒåŸºäºçœŸå®ç»“æœç”ŸæˆæŠ¥å‘Š ----------------------
            st.markdown("### çœŸå®ç»Ÿè®¡åˆ†æç»“æœï¼ˆä¾›AIå‚è€ƒï¼‰")
            st.text(real_stats_text)  # å¯é€‰é¡¹ï¼šå±•ç¤ºçœŸå®ç»“æœç»™ç”¨æˆ·æ ¸å¯¹
            st.markdown("### AIåˆ†æç»“è®ºï¼ˆåŸºäºçœŸå®æ•°æ®ï¼‰")
            
            prompt = f"""ä½ æ˜¯èµ„æ·±ç§‘ç ”ç»Ÿè®¡åˆ†æå¸ˆï¼Œéœ€åŸºäºä»¥ä¸‹**çœŸå®çš„ç»Ÿè®¡ç»“æœ**ç”Ÿæˆåˆ†ææŠ¥å‘Šï¼Œè¦æ±‚ï¼š
1. åªèƒ½ä½¿ç”¨æä¾›çš„çœŸå®ç»Ÿè®¡ç»“æœï¼Œ**ç»å¯¹ä¸èƒ½ç¼–é€ ä»»ä½•æ•°å€¼ã€ç»Ÿè®¡é‡ã€på€¼**ï¼›
2. å…ˆæ€»ç»“æ•°æ®çš„åŸºæœ¬ç‰¹å¾ï¼ˆåŸºäºæè¿°ç»Ÿè®¡ã€é¢‘æ•°ç»“æœï¼‰ï¼›
3. åˆ†æå˜é‡é—´çš„å…³ç³»ï¼ˆåŸºäºç›¸å…³çŸ©é˜µï¼‰ï¼›
4. è§£è¯»ç»Ÿè®¡æ£€éªŒçš„æ„ä¹‰ï¼ˆè‹¥æœ‰å‡å€¼æ£€éªŒç»“æœï¼‰ï¼›
5. æœ€åç»™å‡ºå®¢è§‚çš„åˆ†æç»“è®ºå’Œç ”ç©¶å»ºè®®ï¼›
6. æ ¼å¼æ¸…æ™°ï¼Œåˆ†ç‚¹æ’ç‰ˆï¼Œè¯­è¨€ä¸“ä¸šä¸”æ˜“æ‡‚ã€‚

çœŸå®ç»Ÿè®¡ç»“æœï¼š
{real_stats_text}
"""
            # è°ƒç”¨APIå¹¶æµå¼è¾“å‡º
            stream = call_deepseek_api(prompt)
            st.write_stream(stream)

# æ— æ•°æ®æ—¶çš„æç¤º
else:
    st.info("ğŸ’¡ è¯·åœ¨ã€å·¦ä¾§è¾¹æ ã€‘ä¸Šä¼ CSV/Excelæ•°æ®æ–‡ä»¶ï¼Œå³å¯å¼€å§‹åˆ†æ")
    st.markdown("#### ğŸ“Œ åŠŸèƒ½è¯´æ˜")
    st.markdown("- åŒ…å«SPSSæ ¸å¿ƒç»Ÿè®¡åˆ†æåŠŸèƒ½ï¼Œæ“ä½œæ›´ç®€æ˜“")
    st.markdown("- æ¥å…¥2026ç‰ˆDeepSeek AIï¼Œæ”¯æŒ**è‡ªåŠ¨åˆ†æã€ç»Ÿè®¡é—®ç­”ã€ç»“æœè§£è¯»**ï¼ˆæµå¼è¾“å‡ºé˜²è¶…æ—¶ï¼‰")
    st.markdown("- æ‰€æœ‰åˆ†æç»“æœå¯ç›´æ¥å¤åˆ¶ï¼Œæ”¯æŒå¯è§†åŒ–å›¾è¡¨ç”Ÿæˆ")
